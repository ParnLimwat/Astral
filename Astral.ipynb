{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParnLimwat/Astral/blob/main/Astral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Astral - RNDM Quant Track Notebook**\n",
        "Parn Limwattananon, Philipp Bruhns, Rhin Choe and Xingyan Liu"
      ],
      "metadata": {
        "id": "1xkC_Caqolkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Abstract**"
      ],
      "metadata": {
        "id": "Q_dxlQGMwcGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project employs a Moving Average Crossover Strategy to evaluate trends in cryptocurrency markets, utilizing sentiment scores derived from recent cryptocurrency news. Sentiment analysis and keyword extraction are conducted using Natural Language Processing (NLP) techniques to provide insight into potential market shifts. Backtesting on historical data from the past month reveals a notable profit margin of approximately 30%, underscoring the strategy’s efficacy in short-term cryptocurrency market predictions. ​"
      ],
      "metadata": {
        "id": "BPQdLoEL1sUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Prerequisites**"
      ],
      "metadata": {
        "id": "GLmFq_OIeBNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our code, we're using multiple libraries that require prior installation. We use requests to handle API calls to NewsAPI. Additionally, Pandas and the standard JSON library help us with data manipulation."
      ],
      "metadata": {
        "id": "kiD_K6fyfU1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the NLTK package to perform Natural Language Processing (NLP) on the article's content. By using the vader lexicon, we are able to supply English vocabulary to the model."
      ],
      "metadata": {
        "id": "roFDKtn7j0jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we are using the backtesting.py library as our backtesting engine since it includes crossover and SMA functions which we will use later. It also provides a way to visualise our performance on historic data."
      ],
      "metadata": {
        "id": "9a8F7uxlA3ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install backtesting\n",
        "pip install requests\n",
        "pip install pandas\n",
        "pip install nltk"
      ],
      "metadata": {
        "id": "RVBDm-REfMTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from backtesting import Backtest, Strategy\n",
        "from backtesting.lib import crossover\n",
        "from backtesting.test import SMA\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import json"
      ],
      "metadata": {
        "id": "mVZgyMvgAmER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, also make sure that you have uploaded the ETHSUDT.csv from the repository to the session storage."
      ],
      "metadata": {
        "id": "3S0rmzj1P6z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Data Aggregation**"
      ],
      "metadata": {
        "id": "k2lre6-ejPCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Retrieving news articles via NewsAPI**"
      ],
      "metadata": {
        "id": "l8TC-2pbK1pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to use NewsAPI as the source for our news articles. Unfortunately, the free subscription tier only allows us to retrieve articles from the past month and also limits responses to 100 articles per request. Changing to the paid subscription tier would allow access to over six years and multiple thousand articles per request, which should dramatically improve results. However, the subscription tier does not require any changes in the code below.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sv54S8z_jYfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"ce2f9d4118e044aea7b7f7b57697055c\" # Use the variable key to store a NewsAPI key\n",
        "currency = \"Ethereum\"\n",
        "from_date = \"2024-10-01\"\n",
        "to_date = \"2024-10-25\"\n",
        "\n",
        "# Use the url variable to store the url of the information for news articles related to each cryptocurrency\n",
        "url = (f'https://newsapi.org/v2/everything?q={currency}&from={from_date}&to={to_date}&sortBy=popularity&apiKey={key}')\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "aOWBks1kj5u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, running the code above would give us news articles about 'Ethereum' between 2024-10-01 and 2024-10-25 in JSON format including the total number of results, the articles' sources, the articles' heading, and the first few sentences of the articles.\n",
        "\n"
      ],
      "metadata": {
        "id": "M8GuU1jNkLe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = response.json()\n",
        "\n",
        "articles = data.get('articles')\n",
        "\n",
        "# List of news article headers to analyse\n",
        "contents = [article['content'] for article in articles if article['title'] and article['content']]\n"
      ],
      "metadata": {
        "id": "J-GzyxwklDpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are only interested in the content of each news article for further analysis, we are putting all the contents of the retrieved articles into a list."
      ],
      "metadata": {
        "id": "Z-Q8jeZ6lKBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Historic trading data for backtesting**"
      ],
      "metadata": {
        "id": "D92VRYLuzl8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For later backtesting, we retrieved daily candlebar data of the ETH/USDT trading pair on binance from https://www.CryptoDataDownload.com, which includes data from 2024-10-25 back to 2017-08-17. We then manually reformatted the csv to match the requirements of our backtesting engine and cut the data down to the past month, because the free NewsAPI subscription does not allow us to pull older articles. More on this in **4. Backtesting**.\n",
        "\n",
        "The manually formatted data can be found in ETHUSDT.csv"
      ],
      "metadata": {
        "id": "Po8CHh_QKsXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Strategy: Moving Average Crossover using Sentimental Score**"
      ],
      "metadata": {
        "id": "hT1nnP74jk1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **moving average crossover** is a trading strategy where two moving averages of different periods, a short-term and a long-term average, are used to identify trends. When the short-term moving average crosses above the long-term moving average, it signals a potential uptrend. Conversely, when it crosses below, it indicates a possible downtrend."
      ],
      "metadata": {
        "id": "i6Zmrv8AuNTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our strategy, we decided to use moving average crossovers to analyse trends in the sentiment of news articles about a certain DeFi coin in order to find the points in time where the coin's price is potentially increasing."
      ],
      "metadata": {
        "id": "VWUvQF2JujQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal is to calculate a long-term trend in the sentiment of news articles and check whether the short-term trend is better or worse. If it is better, we want to buy, and if it is worse, we want to sell. Backtesting this strategy on historic data will then let us determine the optimal time interval that should be considered as long-term and short-term respectively (see: **4.1 Optimisation**)."
      ],
      "metadata": {
        "id": "3XNedqnB06s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The strategy consists of four main steps:**\n",
        "1. Use NLP to determine the sentiment of each article.\n",
        "2. Further check for keywords to improve accuracy.\n",
        "3. Calculate an overall sentiment score for a DeFi coin of a specific day based off step 1 and 2.\n",
        "4. Generate buy and sell signals according to the moving average crossing algorithm fed with the sentiment scores from step 3.\n"
      ],
      "metadata": {
        "id": "OqOWzT1z-rqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Natural Language Processing**"
      ],
      "metadata": {
        "id": "S9NOL-wx_Rvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then used a sentiment analyser to determine whether an article delivers a positive or negative sentiment. This analysis can then allow for a polarity score to be assigned, ranging from 0 to 1, where 0 is negative and 1 is positive."
      ],
      "metadata": {
        "id": "7nM1Jq_2xsmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the VADER sentiment analyser\n",
        "sia = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "00nzv6kbkI9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below allows us to obtain the header, sentiment, and sentiment score for each news article."
      ],
      "metadata": {
        "id": "rTHucQ2l8Kcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyse sentiment\n",
        "def analyze_sentiment(headers):\n",
        "    results = []\n",
        "    for header in headers:\n",
        "        sentiment_score = sia.polarity_scores(header)\n",
        "        # Classify as positive, negative, or neutral based on the compound score\n",
        "        if sentiment_score['compound'] >= 0.05:\n",
        "            sentiment = 'Positive'\n",
        "        elif sentiment_score['compound'] <= -0.05:\n",
        "            sentiment = 'Negative'\n",
        "        else:\n",
        "            sentiment = 'Neutral'\n",
        "\n",
        "        results.append((header, sentiment, sentiment_score))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "luP8e4Ig7ukP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These sentiment scores can be used to calculate an overall score. This will be done by the score_on_date function."
      ],
      "metadata": {
        "id": "fIR2iEOJ-Fyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Keywords Analysis**"
      ],
      "metadata": {
        "id": "IwcNBBQZ_Yd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analysing the sentiment of the news articles, we can analyse the positive and negative keywords in the articles to help refine the sentiment score."
      ],
      "metadata": {
        "id": "bV9JhlkZ_q0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_keywords(articles, score):\n",
        "    # Define positive and negative keywords\n",
        "    positive_keywords = ['partnership', 'adoption', 'integration', 'mainstream', 'listing', 'upgrade', 'investment', 'feature']\n",
        "    negative_keywords = ['hack', 'regulatory crackdown', 'ban', 'exit scam', 'fraud', 'shutdown', 'legal issues']\n",
        "\n",
        "    # Check each article for keywords\n",
        "    for article in articles:\n",
        "        content = article.get('content', '').lower()  # Get the content and convert to lowercase\n",
        "\n",
        "        # Check for positive keywords\n",
        "        if any(keyword in content for keyword in positive_keywords):\n",
        "            score += 0.1  # Increase score for each positive keyword found\n",
        "\n",
        "        # Check for negative keywords\n",
        "        if any(keyword in content for keyword in negative_keywords):\n",
        "            score -= 0.1  # Decrease score for each negative keyword found\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "8MyY2ZSk_58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define positive keywords as words that have a connotation with growth, opportunity, stability, or progress. These might include terms like 'partnership,' 'adoption', or 'integration'.\n",
        "\n",
        "Conversely, negative keywords are words that imply risk, uncertainty, loss, or regulatory challenges. Examples could include 'crash,' 'hack,' and 'ban'."
      ],
      "metadata": {
        "id": "YFlYraQ3AACz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each positive keyword in the article, the score would increase by 0.1, while each negative keyword would decrease the score by 0.1."
      ],
      "metadata": {
        "id": "9DN4bkPT__MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Retrieve weighted score on specific a date**"
      ],
      "metadata": {
        "id": "on3NivwiYybY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_on_date(year,month,date,crypto):\n",
        "    date = f\"{year}-{month}-{date}\"\n",
        "    url = (f'https://newsapi.org/v2/everything?q={crypto}&from={date}&to={date}&sortBy=popularity&apiKey=ce2f9d4118e044aea7b7f7b57697055c')\n",
        "\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    articles = data.get('articles')\n",
        "\n",
        "    # List of news article headers to analyse\n",
        "    contents = [article['content'] for article in articles if article['title'] and article['content']]\n",
        "\n",
        "    # Analyse the sentiment of the news headers\n",
        "    analysis_results = analyze_sentiment(contents)\n",
        "\n",
        "    # Initialise counters for sentiment\n",
        "    sum_negative = 0\n",
        "    sum_positive = 0\n",
        "    sum_neutral = 0\n",
        "    total = 0\n",
        "\n",
        "    for header, sentiment, score in analysis_results:\n",
        "        if sentiment == 'Negative':\n",
        "            sum_negative += 1\n",
        "            total += 1\n",
        "        elif sentiment == 'Positive':\n",
        "            sum_positive += 1\n",
        "            total += 1\n",
        "        else:\n",
        "            sum_neutral += 1\n",
        "            total += 1\n",
        "\n",
        "    # Calculate percentage scores\n",
        "    if total > 0:\n",
        "        per_positive = sum_positive / total\n",
        "        per_negative = sum_negative / total\n",
        "        score = per_positive - per_negative\n",
        "    else:\n",
        "        print(f'{crypto} : No sentiment data available.')\n",
        "        score = 0  # No data leads to a neutral score\n",
        "\n",
        "    # Analyse keywords and adjust the score\n",
        "    score = analyze_keywords(articles, score)\n",
        "\n",
        "    return (score)\n"
      ],
      "metadata": {
        "id": "XKOHOF9HXWh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function is used to calculate the sentiment score of the selected cryptocurrency for a given date. This is done by finding the proportion of the articles that are positive and negative and minusing the negative proportions from the positive proportions. The score is then passed into the analyze_keywords function to consider the positive and negative keywords."
      ],
      "metadata": {
        "id": "L7fucPjpXwKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(score_on_date(2024,10,22,\"Ethereum\"))"
      ],
      "metadata": {
        "id": "nwRq46dSZF_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, this displays the score for Ethereum on October 22nd, 2024."
      ],
      "metadata": {
        "id": "MKuW6jm6ZMVL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to not exhaust all of our API Calls, we are using scores that were pre-computed using the same score_on_date function for \"Ethereum\" and save them in a dictionary as shown below.\n",
        "\n",
        "*Note: With access to the premium NewsAPI this is not necessary and would also allow access to scores before the past month.*"
      ],
      "metadata": {
        "id": "_5Fk56nSzxuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlpdict = {\"2024-9-25\": 0.2, \"2024-9-26\": 0.57, \"2024-9-27\": -0.13829787234042556, \"2024-9-28\": 0.13157894736842107, \"2024-9-29\": 0.8428571428571429, \"2024-9-30\": 0.4043478260869565, \"2024-10-1\": 0.53, \"2024-10-2\": 0.9894736842105262, \"2024-10-3\": -1.2, \"2024-10-4\": 0.16853932584269668, \"2024-10-5\": 0.08837209302325583, \"2024-10-6\": -0.20416666666666664, \"2024-10-7\": -0.38, \"2024-10-8\": 0.7941176470588234, \"2024-10-9\": 0.22000000000000006, \"2024-10-10\": 0.8399999999999999, \"2024-10-11\": 0.47777777777777775, \"2024-10-12\": -0.08235294117647063, \"2024-10-13\": 0.5555555555555555, \"2024-10-14\": 1.6400000000000003, \"2024-10-15\": 0.9699999999999999, \"2024-10-16\": 0.702061855670103, \"2024-10-17\": 0.31999999999999995, \"2024-10-18\": 0.47, \"2024-10-19\": 1.0283018867924527, \"2024-10-20\": 0.004545454545454547, \"2024-10-21\": 1.2333333333333334, \"2024-10-22\": 0.4826086956521739, \"2024-10-23\": 0.57, \"2024-10-24\": -0.33, \"2024-10-25\": 0.12000000000000005}\n"
      ],
      "metadata": {
        "id": "dHXNwfzXzwqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Implementing the moving average crossing algorithm**"
      ],
      "metadata": {
        "id": "C5EispHICj5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then implement the moving average crossing algorithm to detect up and down trends in our sentiment score. For this, we are using the built-in crossover and SMA (simple moving average) functions from our backtesting engine."
      ],
      "metadata": {
        "id": "0GjRqGOL9JLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLP(Strategy):\n",
        "    n1 = 6\n",
        "    n2 = 8\n",
        "\n",
        "    def init(self):\n",
        "        self.sma1 = self.I(SMA, nlpdict.values(), self.n1)\n",
        "        self.sma2 = self.I(SMA, nlpdict.values(), self.n2)\n",
        "\n",
        "    def next(self):\n",
        "        if crossover(self.sma1, self.sma2):\n",
        "            self.buy()\n",
        "\n",
        "        elif crossover(self.sma2, self.sma1):\n",
        "            self.sell()"
      ],
      "metadata": {
        "id": "v-FAdhO5w3Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The class variables n1 and n2 determine the length of the short-term and long-term interval respectively. Once the short-term trend crosses the long-term trend, we open a long position. Vice versa, we open a short position if the long-term trend crosses the short-term trend.\n",
        "\n",
        "In this case, we are already using the optimal values for n1 and n2. To see how we determined those see **4.1 Optimisation**."
      ],
      "metadata": {
        "id": "MW-m3o5eDEIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Backtesting**"
      ],
      "metadata": {
        "id": "7wkRY_nj_kUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a proof of concept, we are backtesting our strategy for Ethereum. Specificially, we are simulating it against a binance ETH/USDT as described in **2. Data Aggregation**. Again, using the paid subscription to backtest the past multiple years should improve accuracy by a lot."
      ],
      "metadata": {
        "id": "QcpSZTuPnrtU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load the candlebar data for the ETH/USDT trading pair into a Pandas dataframe. (See: **2. Data Aggregation**)\n",
        "\n",
        "*(Make sure you correctly followed the setup insturctions on GitHub and have uploaded the ETHUSDT.csv to the colab session storage.)*"
      ],
      "metadata": {
        "id": "DJ0UtKefEAyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"ETHUSDT.csv\"\n",
        "prices = pd.read_csv(filename, index_col='Unix', parse_dates=True)"
      ],
      "metadata": {
        "id": "rCJ-to-MC8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we instantiate a backtest for our backtesting engine using this data and the strategy we defined in **3. Strategy Step 4**. The engine simulates with initial equity of 100000 USDT and assumes a commission fee of .001 per trade.\n",
        "By setting exclusive_ordes=True we assume that we only have one open position at a time. This means that opening a new position automatically closes all previous positions first."
      ],
      "metadata": {
        "id": "aSfJdeAkEips"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backtest = Backtest(prices, NLP,\n",
        "              cash=100000, commission=.001,\n",
        "              exclusive_orders=True)\n",
        "output = backtest.run()"
      ],
      "metadata": {
        "id": "c2USplZCEggy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of our backtest can be summarised and plotted using the functions below."
      ],
      "metadata": {
        "id": "kMLyMxBXJhOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "ZO69c1viJUWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "backtest.plot()"
      ],
      "metadata": {
        "id": "VUeGAjiSJJou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see on the plot, the strategy makes a total of 4 trades and yields an overall profit of approximately 31.71% during our one month time span."
      ],
      "metadata": {
        "id": "WEH-W7dWH9G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1 Optimisation**"
      ],
      "metadata": {
        "id": "v-KQN3QUEd-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the optimal values n1 and n2 we can use the built-in optimisation function.\n"
      ],
      "metadata": {
        "id": "4RCzgOQ188tm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stats = backtest.optimize(n1=range(1, 15, 1),\n",
        "                    n2=range(2, 30, 2),\n",
        "                    maximize='Equity Final [$]',\n",
        "                    constraint=lambda param: param.n1 < param.n2)\n",
        "\n",
        "print(stats._strategy)"
      ],
      "metadata": {
        "id": "yXZKNHH0xPP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our case, this gives us n1=6 and n2=8 as optimal values. But as the free NewsAPI subscription limits our available data, the best values may vary on a bigger dataset and with further historic data."
      ],
      "metadata": {
        "id": "brf_CDu5Gqc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Pseudocode**"
      ],
      "metadata": {
        "id": "K10G6E9y6x5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    import necessary libraries\n",
        "    import datetime, timedelta, and date from datetime\n",
        "    import nltk\n",
        "    import SentimentIntensityAnalyzer from nltk.sentiment\n",
        "\n",
        "    use nltk to download vader_lexicon\n",
        "\n",
        "    import requests\n",
        "    import pandas as pd\n",
        "    import json\n",
        "\n",
        "    # Assume we have an API connected to a DeFi trading platform\n",
        "    import TradingAPI\n",
        "    from TradingAPI import crossover, SMA, Strategy\n",
        "---\n",
        "    # Define function to analyze sentiment\n",
        "    FUNCTION analyze_sentiment(contents):\n",
        "      SET results to an empty list\n",
        "\n",
        "      FOR each content in contents:\n",
        "          CALCULATE sentiment_score using sia.polarity_scores(content)\n",
        "\n",
        "          # Classify sentiment based on compound score\n",
        "          IF sentiment_score['compound'] >= 0.05:\n",
        "              SET sentiment to 'Positive'\n",
        "          ELSE IF sentiment_score['compound'] <= -0.05:\n",
        "              SET sentiment to 'Negative'\n",
        "          ELSE:\n",
        "              SET sentiment to 'Neutral'\n",
        "\n",
        "          APPEND (content, sentiment, sentiment_score) to results\n",
        "\n",
        "      RETURN results\n",
        "\n",
        "---\n",
        "    FUNCTION analyze_keywords(articles, score):\n",
        "      DEFINE positive_keywords list with key terms for positive sentiment\n",
        "      DEFINE negative_keywords list with key terms for negative sentiment\n",
        "\n",
        "      FOR each article in articles:\n",
        "          SET content to article's content in lowercase\n",
        "\n",
        "          # Check for positive keywords\n",
        "          IF any keyword in content matches a positive_keywords entry:\n",
        "              INCREMENT score by 0.1\n",
        "\n",
        "          # Check for negative keywords\n",
        "          IF any keyword in content matches a negative_keywords entry:\n",
        "              DECREMENT score by 0.1\n",
        "\n",
        "      RETURN score\n",
        "---\n",
        "    FUNCTION score_on_date(year, month, date, crypto):\n",
        "      SET date to formatted string \"{year}-{month}-{date}\"\n",
        "      SET url to news API endpoint with specified date, crypto, and API key\n",
        "\n",
        "      GET response from API request to url\n",
        "      SET data to parsed JSON response\n",
        "      SET articles to articles within data\n",
        "\n",
        "      # Extract article contents\n",
        "      SET contents to a list of article contents if title and content exist\n",
        "\n",
        "      # Analyze sentiment of extracted contents\n",
        "      SET analysis_results to analyze_sentiment(contents)\n",
        "\n",
        "      # Initialize counters for sentiment categories\n",
        "      SET sum_negative, sum_positive, sum_neutral, total to 0\n",
        "\n",
        "      FOR each (header, sentiment, score) in analysis_results:\n",
        "          IF sentiment is 'Negative':\n",
        "              INCREMENT sum_negative and total by 1\n",
        "          ELSE IF sentiment is 'Positive':\n",
        "              INCREMENT sum_positive and total by 1\n",
        "          ELSE:\n",
        "              INCREMENT sum_neutral and total by 1\n",
        "\n",
        "      # Calculate percentage scores if total is greater than 0\n",
        "      IF total > 0:\n",
        "          SET per_positive to sum_positive / total\n",
        "          SET per_negative to sum_negative / total\n",
        "          SET score to per_positive - per_negative\n",
        "      ELSE:\n",
        "          PRINT \"No sentiment data available for {crypto}\"\n",
        "          SET score to 0\n",
        "\n",
        "      # Adjust score based on keyword analysis\n",
        "      SET score to analyze_keywords(articles, score)\n",
        "\n",
        "      RETURN score\n",
        "\n",
        "\n",
        "---\n",
        "    CLASS NLP inherits from Strategy:\n",
        "        SET n1 to 6\n",
        "        SET n2 to 8\n",
        "\n",
        "        # Define initialization method\n",
        "        FUNCTION init():\n",
        "            SET sma1 to result of calling SMA function on nlpdict values with window size n1\n",
        "            SET sma2 to result of calling SMA function on nlpdict values with window size n2\n",
        "\n",
        "        # Define method for each new candle in the trading strategy\n",
        "        FUNCTION next():\n",
        "            IF crossover(sma1, sma2) is True:\n",
        "                CALL buy()\n",
        "\n",
        "            ELSE IF crossover(sma2, sma1) is True:\n",
        "                CALL sell()\n",
        "\n",
        "\n",
        "    # Main program\n",
        "    DEFINE nlpdict as a dictionary of all historical scores retrieved via score_on_date\n",
        "    nlpdict.sort()\n",
        "\n",
        "    # Execute our strategy on the trading platform\n",
        "    TradingAPI.trade(NLP, exlusive_order=True)"
      ],
      "metadata": {
        "id": "MoMxu2Di69WL"
      }
    }
  ]
}